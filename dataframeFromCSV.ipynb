{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Our First Spark example\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()\n",
    "## ensure to use the link of the raw file of the csv\n",
    "url = \"https://raw.githubusercontent.com/Munasib14/SparkMl-and-SparkSQL/master/SparkMl/HeartStroke.csv\" # Make sure the url is the raw version of the file on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99eb405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:16.050569Z",
     "start_time": "2023-01-24T15:46:01.255796Z"
    }
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "\n",
    "# findspark.init(\"/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark\")\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc= pyspark.SparkContext()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa63c21c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:32.014303Z",
     "start_time": "2023-01-24T15:46:17.169060Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "                         .option(\"header\", \"true\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .load(\"Cars_Sale.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a175dec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:32.076259Z",
     "start_time": "2023-01-24T15:46:32.020259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Vehicle_type: string (nullable = true)\n",
      " |-- Latest_Launch: string (nullable = true)\n",
      " |-- Units_Price: double (nullable = true)\n",
      " |-- Units_Sold : double (nullable = true)\n",
      " |-- Cost_incurred: double (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- Cost: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a894a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:32.140220Z",
     "start_time": "2023-01-24T15:46:32.081258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Manufacturer: string, Model: string, Vehicle_type: string, Latest_Launch: string, Units_Price: double, Units_Sold : double, Cost_incurred: double, Revenue: double, Cost: double, Profit: double]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670d9ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:32.810231Z",
     "start_time": "2023-01-24T15:46:32.146233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-------------+-----------+-----------+-------------+-----------+-----------+-----------+\n",
      "|Manufacturer|      Model|Vehicle_type|Latest_Launch|Units_Price|Units_Sold |Cost_incurred|    Revenue|       Cost|     Profit|\n",
      "+------------+-----------+------------+-------------+-----------+-----------+-------------+-----------+-----------+-----------+\n",
      "|       Acura|    Integra|   Passenger|   02-02-2012|       21.5|     16.919|        16.36|   363.7585|  276.79484|   86.96366|\n",
      "|       Acura|         TL|   Passenger|   06-03-2011|       28.4|     39.384|       19.875|  1118.5056|    782.757|   335.7486|\n",
      "|       Acura|         CL|   Passenger|   01-04-2012|      35.44|     14.114|       18.225|  500.20016|  257.22765|  242.97251|\n",
      "|       Acura|         RL|   Passenger|   03-10-2011|       42.0|      8.588|       29.725|    360.696|   255.2783|   105.4177|\n",
      "|        Audi|         A4|   Passenger|   10-08-2011|      23.99|     20.397|       22.255|  489.32403| 453.935235|  35.388795|\n",
      "|        Audi|         A6|   Passenger|   08-09-2011|      33.95|      18.78|       23.555|    637.581|   442.3629|   195.2181|\n",
      "|        Audi|         A8|   Passenger|    2/27/2012|       62.0|       1.38|         39.0|      85.56|      53.82|      31.74|\n",
      "|         BMW|       323i|   Passenger|    6/28/2011|      26.99|     19.747|         11.0|  532.97153|    217.217|  315.75453|\n",
      "|         BMW|       328i|   Passenger|    1/29/2012|       33.4|      9.231|       28.675|   308.3154| 264.698925|  43.616475|\n",
      "|         BMW|       528i|   Passenger|   04-04-2011|       38.9|     17.527|       36.125|   681.8003| 633.162875|  48.637425|\n",
      "|       Buick|    Century|   Passenger|   11-02-2011|     21.975|     91.561|       12.475|2012.052975|1142.223475|   869.8295|\n",
      "|       Buick|      Regal|   Passenger|   09-03-2011|       25.3|      39.35|        13.74|    995.555|    540.669|    454.886|\n",
      "|       Buick|Park Avenue|   Passenger|    3/23/2012|     31.965|     27.851|        20.19| 890.257215|  562.31169| 327.945525|\n",
      "|       Buick|    LeSabre|   Passenger|    7/23/2011|     27.885|     83.257|        13.36|2321.621445| 1112.31352|1209.307925|\n",
      "|    Cadillac|    DeVille|   Passenger|    2/23/2012|     39.895|     63.729|       22.525|2542.468455|1435.495725| 1106.97273|\n",
      "|    Cadillac|    Seville|   Passenger|    4/29/2011|     44.475|     15.943|         27.1| 709.064925|   432.0553| 277.009625|\n",
      "|    Cadillac|   Eldorado|   Passenger|   11/27/2011|     39.665|      6.536|       25.725|  259.25044|   168.1386|   91.11184|\n",
      "|    Cadillac|     Catera|   Passenger|    9/28/2011|      31.01|     11.185|       18.225|  346.84685| 203.846625| 143.000225|\n",
      "|    Cadillac|   Escalade|         Car|    4/17/2012|     46.225|     14.785|         23.0| 683.436625|    340.055| 343.381625|\n",
      "|   Chevrolet|   Cavalier|   Passenger|    8/17/2011|      13.26|    145.519|         9.25| 1929.58194| 1346.05075|  583.53119|\n",
      "+------------+-----------+------------+-------------+-----------+-----------+-------------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43d19b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:32.950879Z",
     "start_time": "2023-01-24T15:46:32.815222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Vehicle_type: string (nullable = true)\n",
      " |-- Latest_Launch: string (nullable = true)\n",
      " |-- Units_Sold: double (nullable = true)\n",
      " |-- Units_Price: double (nullable = true)\n",
      " |-- Cost_incurred: double (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- Cost: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType, LongType, IntegerType, DateType\n",
    "\n",
    "# define the structure\n",
    "schema = StructType([\n",
    "    StructField(\"Manufacturer\", StringType(),True),\n",
    "    StructField(\"Model\", StringType(),True),\n",
    "    StructField(\"Vehicle_type\", StringType(),True),\n",
    "    StructField(\"Latest_Launch\", StringType(),True),\n",
    "    StructField(\"Units_Sold\", DoubleType(),True),\n",
    "    StructField(\"Units_Price\", DoubleType(),True),\n",
    "    StructField(\"Cost_incurred\", DoubleType(),True),\n",
    "    StructField(\"Revenue\", DoubleType(),True),\n",
    "    StructField(\"Cost\", DoubleType(),True),\n",
    "    StructField(\"Profit\", DoubleType(),True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# read the file by using the defined schema\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"Cars_Sale.csv\")\n",
    "\n",
    "# display the schema\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0ab458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:33.251751Z",
     "start_time": "2023-01-24T15:46:32.956880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-------------+----------+-----------+-------------+-----------+-----------+-----------+\n",
      "|Manufacturer|      Model|Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|    Revenue|       Cost|     Profit|\n",
      "+------------+-----------+------------+-------------+----------+-----------+-------------+-----------+-----------+-----------+\n",
      "|       Acura|    Integra|   Passenger|   02-02-2012|      21.5|     16.919|        16.36|   363.7585|  276.79484|   86.96366|\n",
      "|       Acura|         TL|   Passenger|   06-03-2011|      28.4|     39.384|       19.875|  1118.5056|    782.757|   335.7486|\n",
      "|       Acura|         CL|   Passenger|   01-04-2012|     35.44|     14.114|       18.225|  500.20016|  257.22765|  242.97251|\n",
      "|       Acura|         RL|   Passenger|   03-10-2011|      42.0|      8.588|       29.725|    360.696|   255.2783|   105.4177|\n",
      "|        Audi|         A4|   Passenger|   10-08-2011|     23.99|     20.397|       22.255|  489.32403| 453.935235|  35.388795|\n",
      "|        Audi|         A6|   Passenger|   08-09-2011|     33.95|      18.78|       23.555|    637.581|   442.3629|   195.2181|\n",
      "|        Audi|         A8|   Passenger|    2/27/2012|      62.0|       1.38|         39.0|      85.56|      53.82|      31.74|\n",
      "|         BMW|       323i|   Passenger|    6/28/2011|     26.99|     19.747|         11.0|  532.97153|    217.217|  315.75453|\n",
      "|         BMW|       328i|   Passenger|    1/29/2012|      33.4|      9.231|       28.675|   308.3154| 264.698925|  43.616475|\n",
      "|         BMW|       528i|   Passenger|   04-04-2011|      38.9|     17.527|       36.125|   681.8003| 633.162875|  48.637425|\n",
      "|       Buick|    Century|   Passenger|   11-02-2011|    21.975|     91.561|       12.475|2012.052975|1142.223475|   869.8295|\n",
      "|       Buick|      Regal|   Passenger|   09-03-2011|      25.3|      39.35|        13.74|    995.555|    540.669|    454.886|\n",
      "|       Buick|Park Avenue|   Passenger|    3/23/2012|    31.965|     27.851|        20.19| 890.257215|  562.31169| 327.945525|\n",
      "|       Buick|    LeSabre|   Passenger|    7/23/2011|    27.885|     83.257|        13.36|2321.621445| 1112.31352|1209.307925|\n",
      "|    Cadillac|    DeVille|   Passenger|    2/23/2012|    39.895|     63.729|       22.525|2542.468455|1435.495725| 1106.97273|\n",
      "|    Cadillac|    Seville|   Passenger|    4/29/2011|    44.475|     15.943|         27.1| 709.064925|   432.0553| 277.009625|\n",
      "|    Cadillac|   Eldorado|   Passenger|   11/27/2011|    39.665|      6.536|       25.725|  259.25044|   168.1386|   91.11184|\n",
      "|    Cadillac|     Catera|   Passenger|    9/28/2011|     31.01|     11.185|       18.225|  346.84685| 203.846625| 143.000225|\n",
      "|    Cadillac|   Escalade|         Car|    4/17/2012|    46.225|     14.785|         23.0| 683.436625|    340.055| 343.381625|\n",
      "|   Chevrolet|   Cavalier|   Passenger|    8/17/2011|     13.26|    145.519|         9.25| 1929.58194| 1346.05075|  583.53119|\n",
      "+------------+-----------+------------+-------------+----------+-----------+-------------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f05b976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:33.736133Z",
     "start_time": "2023-01-24T15:46:33.255753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+-------------+----------+---------+------------+\n",
      "|Manufacturer|Vehicle_type|Model  |Latest_Launch|Units_Sold|Revenue  |DefaultValue|\n",
      "+------------+------------+-------+-------------+----------+---------+------------+\n",
      "|Acura       |Passenger   |Integra|02-02-2012   |21.5      |363.7585 |DefaultValue|\n",
      "|Acura       |Passenger   |TL     |06-03-2011   |28.4      |1118.5056|DefaultValue|\n",
      "|Acura       |Passenger   |CL     |01-04-2012   |35.44     |500.20016|DefaultValue|\n",
      "|Acura       |Passenger   |RL     |03-10-2011   |42.0      |360.696  |DefaultValue|\n",
      "+------------+------------+-------+-------------+----------+---------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# select a few columns\n",
    "df1.select(\"Manufacturer\", \"Vehicle_type\", F.col(\"Model\"), \"Latest_Launch\", \"Units_Sold\", \"Revenue\", F.lit('DefaultValue')).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310e1049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.032464Z",
     "start_time": "2023-01-24T15:46:33.741132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "|Manufacturer|Model  |Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|Revenue  |Cost     |Profit   |\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "|Acura       |Integra|Passenger   |02-02-2012   |21.5      |16.919     |16.36        |363.7585 |276.79484|86.96366 |\n",
      "|Acura       |TL     |Passenger   |06-03-2011   |28.4      |39.384     |19.875       |1118.5056|782.757  |335.7486 |\n",
      "|Acura       |CL     |Passenger   |01-04-2012   |35.44     |14.114     |18.225       |500.20016|257.22765|242.97251|\n",
      "|Acura       |RL     |Passenger   |03-10-2011   |42.0      |8.588      |29.725       |360.696  |255.2783 |105.4177 |\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"*\").show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58295f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.063773Z",
     "start_time": "2023-01-24T15:46:34.037204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Manufacturer',\n",
       " 'Model',\n",
       " 'Vehicle_type',\n",
       " 'Latest_Launch',\n",
       " 'Units_Sold',\n",
       " 'Units_Price',\n",
       " 'Cost_incurred',\n",
       " 'Revenue',\n",
       " 'Cost',\n",
       " 'Profit']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1deec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.142966Z",
     "start_time": "2023-01-24T15:46:34.070293Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumnRenamed(\"Vehicle_type\", \"VehicleType\").withColumnRenamed('Units_Sold','units_sold').withColumnRenamed('Units_Price','units_price').withColumnRenamed('Cost_incurred','units_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8540ed6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.363928Z",
     "start_time": "2023-01-24T15:46:34.149960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------+----------+-----------+----------+---------+---------+---------+\n",
      "|Manufacturer|Model  |VehicleType|Latest_Launch|units_sold|units_price|units_cost|Revenue  |Cost     |Profit   |\n",
      "+------------+-------+-----------+-------------+----------+-----------+----------+---------+---------+---------+\n",
      "|Acura       |Integra|Passenger  |02-02-2012   |21.5      |16.919     |16.36     |363.7585 |276.79484|86.96366 |\n",
      "|Acura       |TL     |Passenger  |06-03-2011   |28.4      |39.384     |19.875    |1118.5056|782.757  |335.7486 |\n",
      "|Acura       |CL     |Passenger  |01-04-2012   |35.44     |14.114     |18.225    |500.20016|257.22765|242.97251|\n",
      "+------------+-------+-----------+-------------+----------+-----------+----------+---------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c639b195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.600363Z",
     "start_time": "2023-01-24T15:46:34.370204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "|Manufacturer|Model  |Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|Revenue  |Cost     |Profit   |\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "|Acura       |Integra|Passenger   |02-02-2012   |21.5      |16.919     |16.36        |363.7585 |276.79484|86.96366 |\n",
      "|Acura       |TL     |Passenger   |06-03-2011   |28.4      |39.384     |19.875       |1118.5056|782.757  |335.7486 |\n",
      "|Acura       |CL     |Passenger   |01-04-2012   |35.44     |14.114     |18.225       |500.20016|257.22765|242.97251|\n",
      "|Acura       |RL     |Passenger   |03-10-2011   |42.0      |8.588      |29.725       |360.696  |255.2783 |105.4177 |\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f8f61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:34.962282Z",
     "start_time": "2023-01-24T15:46:34.604357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+--------------+\n",
      "|Manufacturer|Vehicle_type|Model  |Register_Site |\n",
      "+------------+------------+-------+--------------+\n",
      "|Acura       |Passenger   |Integra|www.google.com|\n",
      "|Acura       |Passenger   |TL     |www.google.com|\n",
      "|Acura       |Passenger   |CL     |www.google.com|\n",
      "+------------+------------+-------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding columns to a dataframe\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# add a new column \"Register_Site\" with default value \"www.google.com\"\n",
    "dataDF = df1.withColumn(\"Register_Site\", F.lit(\"www.google.com\"))\n",
    "\n",
    "# display only a few columns\n",
    "dataDF.select(\"Manufacturer\", \"Vehicle_type\",\"Model\", \"Register_Site\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0740ad6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:35.008707Z",
     "start_time": "2023-01-24T15:46:34.965283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns :  11\n",
      "['Manufacturer', 'Model', 'Vehicle_type', 'Latest_Launch', 'Units_Sold', 'Units_Price', 'Cost_incurred', 'Revenue', 'Cost', 'Profit', 'Register_Site']\n",
      "Number of columns :  9\n",
      "['Manufacturer', 'Latest_Launch', 'Units_Sold', 'Units_Price', 'Cost_incurred', 'Revenue', 'Cost', 'Profit', 'Register_Site']\n"
     ]
    }
   ],
   "source": [
    "# removing columns from a DataFrame\n",
    "\n",
    "# number of columns in a dataframe - before removing columns\n",
    "print(\"Number of columns : \", len(dataDF.columns))\n",
    "\n",
    "# columns - before dropping\n",
    "print(list(dataDF.columns))\n",
    "\n",
    "# drop columns - \"Vehicle_type\", \"Model\"\n",
    "datanewDF = dataDF.drop(\"Vehicle_type\", \"Model\")\n",
    "\n",
    "# number of columns in a dataframe - after removing columns\n",
    "print(\"Number of columns : \", len(datanewDF.columns))\n",
    "\n",
    "# columns - after dropping\n",
    "print(list(datanewDF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4cd6d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:35.450988Z",
     "start_time": "2023-01-24T15:46:35.015675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns :  10\n",
      "Number of columns :  11\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+---------+\n",
      "|Manufacturer|  Model|Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|  Revenue|     Cost|   Profit|TotalSale|\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+---------+\n",
      "|       Acura|Integra|   Passenger|   02-02-2012|      21.5|     16.919|        16.36| 363.7585|276.79484| 86.96366| 363.7585|\n",
      "|       Acura|     TL|   Passenger|   06-03-2011|      28.4|     39.384|       19.875|1118.5056|  782.757| 335.7486|1118.5056|\n",
      "|       Acura|     CL|   Passenger|   01-04-2012|     35.44|     14.114|       18.225|500.20016|257.22765|242.97251|500.20016|\n",
      "+------------+-------+------------+-------------+----------+-----------+-------------+---------+---------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# arithmetic with dataframes\n",
    "# number of columns in a dataframe - before a adding a column\n",
    "print(\"Number of columns : \", len(df1.columns))\n",
    "\n",
    "# perform arithmetic operations on a dataframe column\n",
    "newDF = df1.withColumn(\"TotalSale\", col(\"Units_Sold\") * col(\"Units_Price\"))\n",
    "\n",
    "# number of columns in a dataframe - after adding columns\n",
    "print(\"Number of columns : \", len(newDF.columns))\n",
    "\n",
    "# display records\n",
    "newDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee9035d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:35.846532Z",
     "start_time": "2023-01-24T15:46:35.455932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "|Manufacturer|   Model|Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|    Revenue|       Cost|    Profit|\n",
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "|    Cadillac| DeVille|   Passenger|    2/23/2012|    39.895|     63.729|       22.525|2542.468455|1435.495725|1106.97273|\n",
      "|    Cadillac| Seville|   Passenger|    4/29/2011|    44.475|     15.943|         27.1| 709.064925|   432.0553|277.009625|\n",
      "|    Cadillac|Eldorado|   Passenger|   11/27/2011|    39.665|      6.536|       25.725|  259.25044|   168.1386|  91.11184|\n",
      "|    Cadillac|  Catera|   Passenger|    9/28/2011|     31.01|     11.185|       18.225|  346.84685| 203.846625|143.000225|\n",
      "|    Cadillac|Escalade|         Car|    4/17/2012|    46.225|     14.785|         23.0| 683.436625|    340.055|343.381625|\n",
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter a dataframe\n",
    "\n",
    "df1.where(col(\"Manufacturer\") == \"Cadillac\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2e8b768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:46:36.173493Z",
     "start_time": "2023-01-24T15:46:35.854536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "|Manufacturer|   Model|Vehicle_type|Latest_Launch|Units_Sold|Units_Price|Cost_incurred|    Revenue|       Cost|    Profit|\n",
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "|    Cadillac| DeVille|   Passenger|    2/23/2012|    39.895|     63.729|       22.525|2542.468455|1435.495725|1106.97273|\n",
      "|    Cadillac| Seville|   Passenger|    4/29/2011|    44.475|     15.943|         27.1| 709.064925|   432.0553|277.009625|\n",
      "|    Cadillac|Eldorado|   Passenger|   11/27/2011|    39.665|      6.536|       25.725|  259.25044|   168.1386|  91.11184|\n",
      "|    Cadillac|  Catera|   Passenger|    9/28/2011|     31.01|     11.185|       18.225|  346.84685| 203.846625|143.000225|\n",
      "+------------+--------+------------+-------------+----------+-----------+-------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter a dataframe - multiple columns\n",
    "\n",
    "df1.where((col(\"Manufacturer\") == \"Cadillac\") & (col(\"Vehicle_type\") == \"Passenger\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7574eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T15:47:00.923019Z",
     "start_time": "2023-01-24T15:46:44.834644Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o106.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (Munasib14 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m df_new \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(testDF,schema\u001b[38;5;241m=\u001b[39mschema) \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# display the records\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mdf_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o106.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (Munasib14 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "# dropping rows\n",
    "testDF = [[1, \"January\"], [2, \"February\"], [1, \"January\"], [3, \"March\"], [3, \"March\"], [3, \"March\"], [4, \"April\"], [4, \"April\"], [5, \"May\"], [5, \"May\"],\n",
    "          [4, \"April\"], [6, \"June\"], [5, \"April\"]]\n",
    "\n",
    "# import the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"ID\", IntegerType()),StructField(\"Month\", StringType())])\n",
    "\n",
    "# create the dataframe by applying schema\n",
    "df_new = spark.createDataFrame(testDF,schema=schema) \n",
    "\n",
    "# display the records\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde313e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T14:39:30.469914Z",
     "start_time": "2023-01-22T14:39:30.469914Z"
    }
   },
   "outputs": [],
   "source": [
    "# display distinct rows\n",
    "df_new.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b42a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T14:39:30.472533Z",
     "start_time": "2023-01-22T14:39:30.472533Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicate records based a column value\n",
    "df_new.dropDuplicates(['Month']).show()\n",
    "\n",
    "# drop duplicate records based multiple column values\n",
    "df_new.dropDuplicates(['Month', 'ID']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12760de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T14:39:30.475742Z",
     "start_time": "2023-01-22T14:39:30.475742Z"
    }
   },
   "outputs": [],
   "source": [
    "# rename existing columns\n",
    "newDF1 = df1.withColumnRenamed(\"Units_Price\", \"UnitPrice\").withColumnRenamed(\"Profit\", \"Total_Profit\")\n",
    "\n",
    "df1.show(3) # display records\n",
    "\n",
    "from pyspark.sql.functions import expr # define the modules\n",
    "\n",
    "# using select expression \n",
    "newDF1.select(\"Manufacturer\", \"Model\",expr(\"CASE WHEN Total_Profit > 104 THEN  'Good' ELSE 'Average' END AS value_desc\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d4163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T14:39:30.477741Z",
     "start_time": "2023-01-22T14:39:30.477741Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *   # import the libraries\n",
    "\n",
    "# define a list\n",
    "list_data = [[\"Bill Gates\",23],[\"Henry Ford\", None], [\"Tim Cook\", None]]\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"Name\", StringType()),StructField(\"Experience\", IntegerType())])\n",
    "\n",
    "# create a dataframe \n",
    "df_new = spark.createDataFrame(list_data,schema=schema)\n",
    "\n",
    "df_new.show() # display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null value rows\n",
    "df_new.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec911c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null value with a constant value\n",
    "df_new.fillna(34).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace a single value\n",
    "df_new.na.replace('Bill Gates', 'Satya Nadella').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9735af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace multiple values and also fill 'null' with a constant value\n",
    "df_new.na.replace(['Bill Gates', 'Tim Cook'], ['Satya N', 'Time'], 'Name').fillna(40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the existing columns - \"Profit\" to \"Total_Profit\"\n",
    "newDF1 = df1.withColumnRenamed(\"Profit\", \"Total_Profit\")\n",
    "\n",
    "# find maximum total_profit for each region and alias the column to \"Maximum\"\n",
    "newDF1.groupBy(\"Manufacturer\").max(\"Total_Profit\").alias(\"Maximum\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of models by each manufacturer\n",
    "newDF1.groupBy(\"Manufacturer\").agg({'Model':'count'}).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg # include the library\n",
    "\n",
    "# find average of column - \"Total_Profit\" \n",
    "newDF1.select(avg(\"Total_Profit\").alias(\"Average Profit\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the library\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# order the records by manufacturer - ascending\n",
    "df1.orderBy('Manufacturer', ascending=True).select(\"Manufacturer\",\"Model\",\"Vehicle_type\", \"Profit\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the library\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# order the records by manufacturer - desc\n",
    "df1.orderBy('Manufacturer', ascending=False).select(\"Manufacturer\",\"Model\",\"Vehicle_type\", \"Profit\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache and persist\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# cache the dataframe in in-memory\n",
    "cacheDF = df1.cache()\n",
    "\n",
    "# read the records from cache\n",
    "cacheDF.select(\"Manufacturer\", \"Model\", \"Vehicle_type\",  \\\n",
    "               \"Latest_Launch\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache and persist\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# persist the dataframe in both memo\n",
    "persistDF = df1.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# read the records from saved dataframe\n",
    "persistDF.select(\"Manufacturer\", \"Model\", \"Vehicle_type\",  \\\n",
    "               \"Latest_Launch\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce vs repartition\n",
    "print(\"Number of partitions : \", df1.rdd.getNumPartitions())\n",
    "\n",
    "# increase the number of partitions\n",
    "cDF = df1.repartition(2)\n",
    "\n",
    "# number of partitions after repatitioning\n",
    "print(\"Number of partitions : \", cDF.rdd.getNumPartitions())\n",
    "\n",
    "# reduce the number of partitions\n",
    "cDF = cDF.coalesce(1)\n",
    "\n",
    "# number of partitions after coalesce\n",
    "print(\"Number of partitions : \", cDF.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates the Vehicle Type count by Manufacturer, brings the data to a single partition\n",
    "writeDF = newDF1.groupBy(\"Manufacturer\").agg({'Model':'count'}).coalesce(1)  \n",
    "\n",
    "# write to DBFS - mode: \"overwrite\" replaces the existing file and \"append\" adds the content\n",
    "writeDF.write.option(\"header\",\"true\").option(\"sep\",\",\").mode(\"overwrite\").csv(\"/user/glbigdata12/Aggregate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%fs ls \"/user/glbigdata12/Aggregate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "newDF1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "   .load(\"/user/glbigdata12/Aggregate/part-00000-1de425a3-41cf-46d1-8fe8-c74b9d62149f-c000.csv\")\n",
    "\n",
    "# display the records\n",
    "newDF1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43771b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark SQL\n",
    "# create a DataFrame\n",
    "from pyspark.sql.types import *   # import the library\n",
    "leader_data = [[\"Dodge\",\"Mohammed Saif\"],[\"Cadillac\", \"George Carlin\"], \\\n",
    "               [\"BMW\", \"Stuart Broad\"], [\"Ford\", \"Abdalla\"], [\"Hyundai\", \"Chris Gayle\"], \\\n",
    "               [\"Lexus\", \"George Bush\"], [\"Mercury\", \"Tatyaso Martin\"]]\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([StructField(\"Manufacturer\", StringType()), StructField(\"SalesPerson\", StringType())])\n",
    "\n",
    "# create a dataframe and display the records\n",
    "df_new = spark.createDataFrame(leader_data,schema=schema)\n",
    "df_new.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createOrReplaceTempView(\"sales_table\")  # convert dataframe to view\n",
    "\n",
    "# write sql queries using sql()\n",
    "spark.sql(\"select * from sales_table\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sales_table where Manufacturer = 'Cadillac'\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sales_table where SalesPerson like '%George%'\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from sales_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"vehicle\")\n",
    "\n",
    "spark.sql(\"select * from vehicle\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a column using DSL\n",
    "newDF1 = df1.withColumnRenamed(\"Revenue\", \"TotalRevenue\")\n",
    "\n",
    "# create a temp view\n",
    "\n",
    "newDF1.createOrReplaceTempView(\"vehicle\")\n",
    "\n",
    "# apply aggregations on the table data\n",
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer order by Manufacturer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select Manufacturer, max(TotalRevenue) from vehicle group by Manufacturer order by Manufacturer desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a70799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join (inner) vehicle and sales_table, display the results\n",
    "spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af97931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join (inner) vehicle and sales_table, apply a where condition, display the results\n",
    "df_new = spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\n",
    "       where trim(a.Manufacturer) = \"Cadillac\"\n",
    "       \"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b50f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results in to DBFS\n",
    "df_new = spark.sql(\"\"\"select a.Manufacturer, a.Model, b.SalesPerson\n",
    "       from vehicle a\n",
    "       join sales_table b\n",
    "       on trim(a.Manufacturer) = trim(b.Manufacturer)\n",
    "       where trim(a.Manufacturer) = \"Cadillac\"\n",
    "       \"\"\")\n",
    "\n",
    "\n",
    "df_new.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").csv(\"/user/glbigdata12/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da094051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%fs ls \"/user/glbigdata12/spark/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b2b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d8352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da73ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a319d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7fe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16eaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8537c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72089d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa54ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958706a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a2812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
